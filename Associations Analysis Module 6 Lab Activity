Roxana Villagomez
Module 6: Lab Activity

import pandas as pd
import numpy as np
from mlxtend.frequent_patterns import apriori, association_rules
import matplotlib.pyplot as plt
## Use this to read data directly from github
df = pd.read_csv('https://gist.githubusercontent.com/Harsh-Git-Hub/2979ec48043928ad9033d8469928e751/raw/72de943e040b8bd0d087624b154d41b2ba9d9b60/retail_dataset.csv', sep=',')
## Use this to read data from the csv file on local system.
df = pd.read_csv('./data/retail_data.csv', sep=',') 
## Print first 10 rows 
df.head(10)

Each row of the dataset represents items that were purchased together on the same day at the same store.The dataset is a sparse dataset as relatively high percentage of data is NA or NaN or equivalent.

These NaNs make it hard to read the table. Let’s find out how many unique items are actually there in the table.

items = set()
for col in df:
    items.update(df[col].unique())
print(items)
Out:
{'Bread', 'Cheese', 'Meat', 'Eggs', 'Wine', 'Bagel', 'Pencil',
       'Diaper', 'Milk']}
There are only 9 items in total that make up the entire dataset. Nice, easier to find frequent itemset or so you think!!

Data Preprocessing
To make use of the apriori module given by mlxtend library, we need to convert the dataset according to it’s liking. apriori module requires a dataframe that has either 0 and 1 or True and False as data. The data we have is all string (name of items), we need to One Hot Encode the data.

Custom One Hot Encoding

itemset = set(items)
encoded_vals = []
for index, row in df.iterrows():
    rowset = set(row) 
    labels = {}
    uncommons = list(itemset - rowset)
    commons = list(itemset.intersection(rowset))
    for uc in uncommons:
        labels[uc] = 0
    for com in commons:
        labels[com] = 1
    encoded_vals.append(labels)
encoded_vals[0]
ohe_df = pd.DataFrame(encoded_vals)
Applying Apriori
apriori module from mlxtend library provides fast and efficient apriori implementation.

apriori(df, min_support=0.5, use_colnames=False, max_len=None, verbose=0, low_memory=False)

Parameters

df : One-Hot-Encoded DataFrame or DataFrame that has 0 and 1 or True and False as values
min_support : Floating point value between 0 and 1 that indicates the minimum support required for an itemset to be selected.
# of observation with item / total observation# of observation with item / total observation
use_colnames : This allows to preserve column names for itemset making it more readable.
max_len : Max length of itemset generated. If not set, all possible lengths are evaluated.
verbose : Shows the number of iterations if >= 1 and low_memory is True. If =1 and low_memory is False , shows the number of combinations.
low_memory :
If True, uses an iterator to search for combinations above min_support. Note that while low_memory=True should only be used for large dataset if memory resources are limited, because this implementation is approx. 3–6x slower than the default.
freq_items = apriori(ohe_df, min_support=0.2, use_colnames=True, verbose=1)
freq_items.head(7)

The output is a data frame with the support for each itemsets.

Mining Association Rules
Frequent if-then associations called association rules which consists of an antecedent (if) and a consequent (then).

association_rules(df, metric=’confidence’, min_threshold=0.8, support_only=False)

Metric can be set to confidence, lift, support, leverage and conviction.

rules = association_rules(freq_items, metric="confidence", min_threshold=0.6)
rules.head()

The result of association analysis shows which item is frequently purchased with other items.

Visualizing results
Support vs Confidence
plt.scatter(rules['support'], rules['confidence'], alpha=0.5)
plt.xlabel('support')
plt.ylabel('confidence')
plt.title('Support vs Confidence')
plt.show()

2. Support vs Lift

plt.scatter(rules[‘support’], rules[‘lift’], alpha=0.5)
plt.xlabel(‘support’)
plt.ylabel(‘lift’)
plt.title(‘Support vs Lift’)
plt.show()

Lift vs Confidence

fit = np.polyfit(rules[‘lift’], rules[‘confidence’], 1)
fit_fn = np.poly1d(fit)
plt.plot(rules[‘lift’], rules[‘confidence’], ‘yo’, rules[‘lift’], 
 fit_fn(rules[‘lift’]))

